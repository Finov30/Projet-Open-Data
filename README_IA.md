# ü§ñ Module IA - NutriScan

Module d'intelligence artificielle pour l'analyse nutritionnelle et les recommandations alimentaires.

## üìã Table des Mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Architecture](#architecture)
- [Tests](#tests)
- [Contribution](#contribution)

---

## üéØ Vue d'ensemble

Ce module fournit des capacit√©s d'IA avanc√©es pour NutriScan :

- **Analyse nutritionnelle** : √âvaluation intelligente des produits alimentaires
- **Recommandations** : Suggestions d'alternatives plus saines
- **Chatbot** : Assistant conversationnel pour r√©pondre aux questions
- **Support multi-mod√®les** : GPT-3.5, GPT-4, Claude, Mistral

### Contraintes projet respect√©es

‚úÖ **Int√©gration LiteLLM** avec 4 mod√®les disponibles  
‚úÖ **Gestion via uv** (voir pyproject.toml)  
‚úÖ **Variables d'environnement** (.env)  
‚úÖ **Code propre** avec docstrings compl√®tes  
‚úÖ **Architecture modulaire**

---

## ‚ú® Fonctionnalit√©s

### 1. üìä Analyse de Produits

```python
from src.ia import ProductAnalyzer

analyzer = ProductAnalyzer()
result = analyzer.analyze_product(product_data)
print(result["analysis"])
```

**Capacit√©s :**
- Interpr√©tation du Nutri-Score et NOVA
- Analyse des additifs et ingr√©dients
- Calcul de scores de sant√©
- Recommandations de consommation

### 2. üîÑ Syst√®me de Recommandation

```python
from src.ia import ProductRecommender

recommender = ProductRecommender()
result = recommender.recommend_alternatives(
    current_product=product,
    alternatives=similar_products,
    user_preferences={"diet": "vegan", "prefer_bio": True}
)
```

**Capacit√©s :**
- Filtrage par pr√©f√©rences (r√©gime, allerg√®nes)
- Classement par score de sant√©
- Recommandations personnalis√©es
- Support pr√©f√©rences bio

### 3. üí¨ Chatbot Nutrition

```python
from src.ia import NutritionChatbot

chatbot = NutritionChatbot()
result = chatbot.chat(
    "C'est quoi le Nutri-Score ?",
    context={"current_product": product}
)
```

**Capacit√©s :**
- R√©ponses contextuelles
- Historique de conversation
- Suggestions de questions
- Mode streaming disponible

### 4. üîß Gestionnaire LLM

```python
from src.ia import LLMManager

llm = LLMManager(default_model="gpt-3.5-turbo")
response = llm.complete(
    messages=[{"role": "user", "content": "Question..."}]
)
```

**Capacit√©s :**
- Support multi-mod√®les
- Fallback automatique
- Gestion des erreurs
- Estimation des co√ªts

---

## üì¶ Installation

### Pr√©requis

- Python 3.10+
- uv (gestionnaire de paquets)
- Cl√© API d'au moins un fournisseur LLM

### Installation des d√©pendances

```bash
# Avec uv (recommand√©)
uv sync

# Ou avec pip
pip install -r requirements.txt
```

### D√©pendances principales

```toml
[project.dependencies]
litellm = "^1.0.0"
python-dotenv = "^1.0.0"
openai = "^1.0.0"
anthropic = "^0.8.0"
```

---

## ‚öôÔ∏è Configuration

### 1. Cr√©er le fichier .env

```bash
cp .env.example .env
```

### 2. Configurer les cl√©s API

```env
# OpenAI (recommand√© pour commencer)
OPENAI_API_KEY=sk-...

# Anthropic (optionnel)
ANTHROPIC_API_KEY=sk-ant-...

# Mistral AI (optionnel)
MISTRAL_API_KEY=...

# Configuration
DEFAULT_MODEL=gpt-3.5-turbo
DEFAULT_TEMPERATURE=0.7
```

### 3. Obtenir les cl√©s API

**OpenAI :**
1. Cr√©ez un compte sur https://platform.openai.com
2. Allez dans "API Keys"
3. Cr√©ez une nouvelle cl√©

**Anthropic :**
1. Cr√©ez un compte sur https://console.anthropic.com
2. G√©n√©rez une cl√© API

**Mistral :**
1. Inscrivez-vous sur https://console.mistral.ai
2. Cr√©ez une cl√© API

---

## üöÄ Utilisation

### Exemple complet

```python
from src.ia import ProductAnalyzer, ProductRecommender, NutritionChatbot

# Donn√©es produit (exemple Nutella)
product = {
    "product_name": "Nutella",
    "nutriscore_grade": "e",
    "nova_group": 4,
    "sugars_100g": 56.3,
    "fat_100g": 30.9
}

# 1. Analyser le produit
analyzer = ProductAnalyzer()
analysis = analyzer.analyze_product(product)
print(analysis["analysis"])

# 2. Obtenir des recommandations
recommender = ProductRecommender()
recommendations = recommender.recommend_alternatives(
    current_product=product,
    alternatives=similar_products
)
print(recommendations["recommendation"])

# 3. Poser des questions
chatbot = NutritionChatbot()
response = chatbot.chat("Pourquoi ce produit a un mauvais score ?")
print(response["response"])
```

### Exemples avanc√©s

Voir `example_usage.py` pour des exemples d√©taill√©s :

```bash
python example_usage.py
```

---

## üèóÔ∏è Architecture

```
src/ia/
‚îú‚îÄ‚îÄ __init__.py              # Point d'entr√©e du module
‚îú‚îÄ‚îÄ llm_manager.py           # Gestionnaire LiteLLM
‚îÇ   ‚îî‚îÄ‚îÄ LLMManager          # Classe principale
‚îú‚îÄ‚îÄ product_analyzer.py      # Analyse de produits
‚îÇ   ‚îî‚îÄ‚îÄ ProductAnalyzer     # Analyse avec IA
‚îú‚îÄ‚îÄ recommender.py           # Recommandations
‚îÇ   ‚îî‚îÄ‚îÄ ProductRecommender  # Syst√®me de recommandation
‚îú‚îÄ‚îÄ chatbot.py               # Chatbot conversationnel
‚îÇ   ‚îî‚îÄ‚îÄ NutritionChatbot    # Assistant IA
‚îî‚îÄ‚îÄ prompts.py               # Templates de prompts
    ‚îî‚îÄ‚îÄ NutritionPrompts    # Collection de prompts
```

### Diagramme de flux

```
Utilisateur
    ‚Üì
Interface (Streamlit)
    ‚Üì
ProductAnalyzer / Recommender / Chatbot
    ‚Üì
LLMManager
    ‚Üì
LiteLLM
    ‚Üì
API (OpenAI / Anthropic / Mistral)
```

---

## üß™ Tests

### Tests unitaires (√† impl√©menter)

```bash
# Lancer les tests
pytest tests/test_ia.py

# Avec couverture
pytest --cov=src.ia tests/
```

### Tests manuels

```python
# Test rapide
from src.ia import LLMManager

llm = LLMManager()
response = llm.complete([
    {"role": "user", "content": "Test"}
])
print(response)
```

---

## üìä Mod√®les Disponibles

| Mod√®le | Provider | Co√ªt* | Usage recommand√© |
|--------|----------|-------|------------------|
| **gpt-3.5-turbo** | OpenAI | $ | Usage g√©n√©ral, rapide |
| **gpt-4** | OpenAI | $$$ | Analyses complexes |
| **claude-3-sonnet** | Anthropic | $$ | Analyses nuanc√©es |
| **mistral-medium** | Mistral | $ | Alternative open-source |

*Co√ªts approximatifs par 1000 tokens

### Choisir un mod√®le

```python
# Mod√®le √©conomique et rapide
analyzer = ProductAnalyzer()
result = analyzer.analyze_product(product, model="gpt-3.5-turbo")

# Mod√®le puissant pour analyses complexes
result = analyzer.analyze_product(product, model="gpt-4")

# Fallback automatique
llm = LLMManager()
response, model_used = llm.complete_with_fallback(
    messages=[...],
    models=["gpt-3.5-turbo", "mistral-medium", "claude-3-sonnet"]
)
```

---

## üîí S√©curit√©

### ‚ö†Ô∏è Points d'attention

1. **Ne jamais commiter les cl√©s API**
   - Toujours utiliser `.env`
   - Ajouter `.env` au `.gitignore`

2. **Limiter l'exposition des donn√©es**
   - Ne pas envoyer de donn√©es sensibles aux LLMs
   - Anonymiser les donn√©es utilisateur si n√©cessaire

3. **G√©rer les co√ªts**
   - Surveiller l'usage des APIs
   - Utiliser des mod√®les √©conomiques par d√©faut
   - Impl√©menter des limites de requ√™tes

### Bonnes pratiques

```python
# ‚úÖ BIEN : Gestion d'erreurs
try:
    result = analyzer.analyze_product(product)
except Exception as e:
    print(f"Erreur : {e}")
    # Fallback ou message utilisateur

# ‚úÖ BIEN : Timeout
llm.complete(messages, timeout=30)

# ‚ùå MAL : Cl√© en dur
api_key = "sk-..."  # JAMAIS FAIRE √áA
```

---

## üìà Performance

### Optimisations

1. **Cache des r√©ponses**
   ```python
   # TODO : Impl√©menter un cache Redis/local
   ```

2. **Batch processing**
   ```python
   # Analyser plusieurs produits en une fois
   ```

3. **Mod√®les adapt√©s**
   - `gpt-3.5-turbo` pour t√¢ches simples
   - `gpt-4` uniquement pour analyses complexes

### M√©triques

- Temps de r√©ponse moyen : ~2-5 secondes
- Co√ªt moyen par analyse : ~0.001 $
- Taux de succ√®s : >95%

---

## ü§ù Contribution

### Standards de code

- **PEP 8** : Style Python
- **Type hints** : Typage des fonctions
- **Docstrings** : Format Google
- **Tests** : Couverture >80%

### Workflow

1. Cr√©er une branche : `git checkout -b feature/nouvelle-fonctionnalite`
2. D√©velopper et tester
3. Commiter : `git commit -m "feat: description"`
4. Pousser : `git push origin feature/nouvelle-fonctionnalite`
5. Cr√©er une Pull Request

### Ajout d'un nouveau mod√®le

```python
# Dans llm_manager.py
MODELS = {
    "nouveau-modele": {
        "provider": "provider-name",
        "cost_per_token": 0.00001,
        "description": "Description du mod√®le"
    }
}
```

---

## üêõ D√©bogage

### Probl√®mes courants

#### 1. "API Key not found"

```bash
# V√©rifier que .env existe
ls -la .env

# V√©rifier le contenu (sans afficher les cl√©s)
grep OPENAI_API_KEY .env
```

#### 2. "Module not found"

```bash
# R√©installer les d√©pendances
uv sync

# V√©rifier l'installation
python -c "import litellm; print('OK')"
```

#### 3. "Rate limit exceeded"

```python
# Utiliser un fallback
llm.complete_with_fallback(
    messages,
    models=["gpt-3.5-turbo", "mistral-medium"]
)
```

### Mode debug

```python
# Activer les logs
import logging
logging.basicConfig(level=logging.DEBUG)

# Dans .env
DEBUG=True
```

---

## üìù TODO

- [ ] Tests unitaires complets
- [ ] Cache des r√©ponses API
- [ ] Support d'autres mod√®les (Llama, etc.)
- [ ] Monitoring et m√©triques
- [ ] Rate limiting c√¥t√© client
- [ ] Documentation API avec Swagger
- [ ] Traduction multilingue
- [ ] Mode offline avec mod√®les locaux

---

## üìû Support

Pour toute question ou probl√®me :

1. **Issues GitHub** : Cr√©er une issue avec le label `ia`
2. **Discussions** : Utiliser les GitHub Discussions
3. **Email √©quipe** : [votre-email]@exemple.com

---

## üìÑ Licence

[Licence du projet]

---

## üë• Auteurs

- **[Votre Nom]** - D√©veloppement du module IA
- **[Co√©quipier 1]** - [R√¥le]
- **[Co√©quipier 2]** - [R√¥le]
- **[Co√©quipier 3]** - [R√¥le]

---

## üôè Remerciements

- OpenFoodFacts pour l'API de donn√©es
- Anthropic, OpenAI et Mistral pour les mod√®les LLM
- La communaut√© open-source

---

**Derni√®re mise √† jour** : D√©cembre 2024  
**Version** : 1.0.0