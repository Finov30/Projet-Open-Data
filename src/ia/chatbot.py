"""
Chatbot conversationnel pour r√©pondre aux questions sur la nutrition.
"""
from typing import Dict, List, Optional, Any
from .llm_manager import LLMManager
from .prompts import NutritionPrompts

class NutritionChatbot:
    """Chatbot intelligent pour r√©pondre aux questions nutritionnelles."""
    
    def __init__(self, llm_manager: Optional[LLMManager] = None):
        """
        Initialise le chatbot.
        
        Args:
            llm_manager: Instance du gestionnaire LLM
        """
        self.llm = llm_manager or LLMManager()
        self.prompts = NutritionPrompts()
        self.conversation_history: List[Dict[str, str]] = []
    
    def chat(
        self,
        user_message: str,
        context: Optional[Dict[str, Any]] = None,
        model: str = "gpt-3.5-turbo",
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        R√©pond √† un message utilisateur.
        
        Args:
            user_message: Message de l'utilisateur
            context: Contexte additionnel (produit en cours de consultation, etc.)
            model: Mod√®le LLM √† utiliser
            stream: Si True, retourne un g√©n√©rateur pour streaming
            
        Returns:
            R√©ponse du chatbot
        """
        # Pr√©pare le contexte
        context_str = ""
        if context:
            if "current_product" in context:
                product = context["current_product"]
                context_str = f"\n**Produit en consultation :** {product.get('product_name', 'Inconnu')}"
                context_str += f"\nNutri-Score: {product.get('nutriscore_grade', '?')}"
        
        # Construit l'historique de conversation
        messages = [
            {"role": "system", "content": self.prompts.chatbot_system_prompt()}
        ]
        
        # Ajoute l'historique (derniers 10 messages pour √©viter de d√©passer le contexte)
        messages.extend(self.conversation_history[-10:])
        
        # Ajoute le message actuel avec contexte
        user_content = user_message
        if context_str:
            user_content += context_str
        
        messages.append({"role": "user", "content": user_content})
        
        try:
            response = self.llm.complete(
                messages=messages,
                model=model,
                temperature=0.7,
                max_tokens=400,
                stream=stream
            )
            
            # Si streaming, on retourne le g√©n√©rateur directement
            if stream:
                return {
                    "success": True,
                    "stream": response,
                    "model_used": model
                }
            
            # Sinon, on met √† jour l'historique
            self.conversation_history.append({"role": "user", "content": user_message})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            return {
                "success": True,
                "response": response,
                "model_used": model,
                "message_count": len(self.conversation_history)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def clear_history(self):
        """Efface l'historique de conversation."""
        self.conversation_history = []
    
    def get_history(self) -> List[Dict[str, str]]:
        """Retourne l'historique de conversation."""
        return self.conversation_history.copy()
    
    def ask_about_ingredient(
        self,
        ingredient: str,
        model: str = "gpt-3.5-turbo"
    ) -> Dict[str, Any]:
        """
        Pose une question sp√©cifique sur un ingr√©dient.
        
        Args:
            ingredient: Nom de l'ingr√©dient
            model: Mod√®le LLM √† utiliser
            
        Returns:
            Explication de l'ingr√©dient
        """
        question = f"Peux-tu m'expliquer ce qu'est {ingredient} et s'il faut l'√©viter ?"
        return self.chat(question, model=model)
    
    def ask_about_allergen(
        self,
        allergen: str,
        model: str = "gpt-3.5-turbo"
    ) -> Dict[str, Any]:
        """
        Pose une question sur un allerg√®ne.
        
        Args:
            allergen: Nom de l'allerg√®ne
            model: Mod√®le LLM √† utiliser
            
        Returns:
            Informations sur l'allerg√®ne
        """
        question = f"Je suis allergique au {allergen}. Quels produits dois-je √©viter et quelles sont les alternatives ?"
        return self.chat(question, model=model)
    
    def get_quick_answer(
        self,
        question: str,
        model: str = "gpt-3.5-turbo"
    ) -> str:
        """
        Obtient une r√©ponse rapide sans historique.
        
        Args:
            question: Question de l'utilisateur
            model: Mod√®le LLM √† utiliser
            
        Returns:
            R√©ponse textuelle
        """
        messages = [
            {"role": "system", "content": self.prompts.chatbot_system_prompt()},
            {"role": "user", "content": question}
        ]
        
        try:
            response = self.llm.complete(
                messages=messages,
                model=model,
                temperature=0.7,
                max_tokens=200
            )
            return response
        except Exception as e:
            return f"D√©sol√©, une erreur s'est produite : {str(e)}"
    
    def suggest_questions(
        self,
        context: Optional[Dict[str, Any]] = None
    ) -> List[str]:
        """
        Sugg√®re des questions pertinentes selon le contexte.
        
        Args:
            context: Contexte actuel (produit consult√©, etc.)
            
        Returns:
            Liste de questions sugg√©r√©es
        """
        base_questions = [
            "üçé C'est quoi le Nutri-Score exactement ?",
            "üî¨ Qu'est-ce qu'un additif alimentaire ?",
            "ü•ó Comment composer un repas √©quilibr√© ?",
            "‚ö†Ô∏è Quels sont les allerg√®nes les plus courants ?",
            "üè∑Ô∏è C'est quoi un produit bio ?"
        ]
        
        # Ajoute des questions contextuelles si un produit est consult√©
        if context and "current_product" in context:
            product = context["current_product"]
            
            contextual_questions = []
            
            # Questions sur le Nutri-Score
            nutriscore = product.get("nutriscore_grade", "").upper()
            if nutriscore:
                contextual_questions.append(
                    f"Pourquoi ce produit a un Nutri-Score {nutriscore} ?"
                )
            
            # Questions sur les additifs
            additives = product.get("additives_tags", [])
            if additives:
                contextual_questions.append(
                    "Les additifs de ce produit sont-ils dangereux ?"
                )
            
            # Questions sur les allerg√®nes
            allergens = product.get("allergens_tags", [])
            if allergens:
                contextual_questions.append(
                    "Quels sont les allerg√®nes pr√©sents dans ce produit ?"
                )
            
            # Questions sur le NOVA
            nova = product.get("nova_group")
            if nova:
                contextual_questions.append(
                    f"C'est grave si un produit est NOVA {nova} ?"
                )
            
            return contextual_questions + base_questions
        
        return base_questions
    
    def explain_score(
        self,
        score_type: str,
        score_value: Any,
        product_name: str = "",
        model: str = "gpt-3.5-turbo"
    ) -> Dict[str, Any]:
        """
        Explique un score nutritionnel sp√©cifique.
        
        Args:
            score_type: Type de score (nutriscore, nova, etc.)
            score_value: Valeur du score
            product_name: Nom du produit (optionnel)
            model: Mod√®le LLM √† utiliser
            
        Returns:
            Explication du score
        """
        product_str = f" pour {product_name}" if product_name else ""
        
        question = f"Peux-tu m'expliquer en d√©tail le score {score_type.upper()} {score_value}{product_str} ?"
        
        return self.chat(question, model=model)